# Voice LLMs Assistant

A voice-based AI assistant that creates a natural conversation flow by combining three powerful AI models:

üé§ Groq Whisper - Converts your speech to text
üß† Groq LLaMA - Processes and understands your message
üîä OpenAI TTS - Converts the AI response back to speech

## Project Structure
```
voice-llms/
‚îú‚îÄ‚îÄ voice_llms.py        # Main functionality
‚îî‚îÄ‚îÄ requirements.txt     # Project dependencies
```

## Features

- üé§ Real-time voice recording
- üó£Ô∏è Speech-to-Text using Groq's Whisper Large v3
- ü§ñ LLM processing using Groq's LLaMA 3-8b model
- üîä Text-to-Speech using OpenAI's TTS
- ‚ñ∂Ô∏è Immediate audio playback of responses

## Quick Start

1. **Clone the repository**
```bash
git clone [your-repository-url]
cd voice-llms
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up API Keys**

You have two options to set up your API keys:

Option 1: Direct in code (Quick testing)
```python
# In voice_llms.py
groq_client = Groq(api_key="your_groq_api_key")
openai_client = OpenAI(api_key="your_openai_api_key")
```

Option 2: Environment variables (Recommended for development)
```bash
# Export in terminal
export GROQ_API_KEY="your_groq_api_key"
export OPENAI_API_KEY="your_openai_api_key"

# Or create .env file
GROQ_API_KEY=your_groq_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
```

4. **Run the assistant**

Simple run:
```bash
python voice_llms.py
```

Or import in your code:
```python
from voice_llms import record_and_process

# Start recording and processing
result = record_and_process(duration=3)  # Records for 3 seconds
```

## Usage Examples

### Basic Usage
```python
# Direct run
python voice_llms.py

# Or import and use
from voice_llms import record_and_process

# Record and process speech
result = record_and_process()

# Access results
print(f"You said: {result['transcription']}")
print(f"AI responded: {result['ai_response']}")
```

### Custom Duration Recording
```python
# Record for 5 seconds
result = record_and_process(duration=5)
```

### Custom Frequency
```python
# Record with different sampling frequency
result = record_and_process(freq=48000)
```

### Custom Output Filename
```python
# Specify custom output filename
result = record_and_process(filename="my_recording.wav")
```

## Dependencies

- sounddevice: Audio recording
- scipy: Audio processing
- wavio: WAV file handling
- pydub: Audio manipulation
- pygame: Audio playback
- groq: Whisper STT and LLaMA
- openai: TTS service
- python-dotenv: Environment management (optional)

## Configuration

### LLM Parameters
The LLaMA model can be configured by modifying parameters in `voice_llms.py`:
```python
chat_completion = groq_client.chat.completions.create(
    model="llama3-8b-8192",
    temperature=0.5,
    max_tokens=2048,
    top_p=1
)
```

### TTS Voice Options
Change the OpenAI TTS voice in `voice_llms.py`:
```python
tts_response = openai_client.audio.speech.create(
    model="tts-1",
    voice="nova",  # Options: alloy, echo, fable, onyx, nova, shimmer
    input=ai_response
)
```

## Error Handling

Common errors and solutions:

1. **Audio device not found**
   - Ensure microphone is connected
   - Check system audio settings

2. **API Key errors**
   - Verify API keys are correct
   - If using environment variables, ensure they're properly set
   - If using direct keys, check for typos

3. **Audio playback issues**
   - Ensure speakers/headphones are connected
   - Check pygame initialization

## License

MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [Groq](https://groq.com/) for Whisper and LLaMA models
- [OpenAI](https://openai.com/) for TTS service
- All open-source package contributors